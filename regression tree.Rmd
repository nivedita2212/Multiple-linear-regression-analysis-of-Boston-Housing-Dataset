---
title: "Regression_tree"
author: "Nivedita"
date: "2024-04-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

To perform fitting of regression tree, bagging, boosting and random forest for Boston dataset in MASS package.

## Analysis

```{r ,echo=F}
## Call MASS library
library(MASS)
```
```{r ,echo=T}
# Show number of rows and columns of the Boston dataset
dim(Boston)
```

In the context of R, the Boston dataset is found in the MASS library and has 506 rows and 14 columns.

```{r, echo=TRUE}
# Show first 6 rows of data
head(Boston)
## Print out the column names of Boston dataset using names function
names(Boston)
```

### Fitting of regression tree

We want to fit regression tree for this dataset to predict median value of a home, medv. First, we create a training set, and fit the tree to the training data.

```{r echo=TRUE}
library(tree)
set.seed(1)
train <-sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <-tree(medv~., Boston, subset = train)
summary(tree.boston)
```
From summary we can see that only four of the variables have been used in constructing the tree that are "rm", "lstat", "crim" , "age" . For visual representation of fitted regression tree we use plot command.

```{r echo=TRUE}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

*Fig. 1 Plot of regression tree*

The variable lstat measures the percentage of individuals with lower socioeconomic status, while the variable rm corresponds to the average num
ber of rooms. The tree indicates that larger values of rm, or lower values of lstat, correspond to more expensive houses. For example, the tree predicts a median house price of $45,400 for homes in census tracts in which rm >=7.553.


Now we use the cv.tree() function to see whether pruning the tree will improve performance.

```{r echo=TRUE}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b",main='Plot of error rate function of size ')
```

*Fig. 2 Plot of error rate function of size *

Form Fig. 2 we can see that we can see that 7 terminal nodes results minnimum cross validation errors and we have 7 terminal nodes in the fitted regression tree. So  in this case, the most complex tree under consideration is selected by cross validation that means there is no need to perform prining process.


In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.

```{r echo=TRUE}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test,xlab='Predicted value of medv',ylab = 'Vales of medv in test data', main='Plot of observed and predicted value of medv for test data' )
abline(0, 1)
```

*Fig. 3 Plot of observed and predicted value of medv for test data*

```{r echo=TRUE}
mean((yhat- boston.test)^2)
```


We can see that the test set MSE associated with the regression tree is 35.29. The square root of the MSE is therefore around 5.941 indicates that  this model leads to test predictions that are (on average) within approximately $5,941 of the true median home value for the census tract.

### Bagging

Here we apply bagging to the Boston data, using the randomForest package in R. The argument mtry = 12 indicates that all 12 predictors should be considered for each split of the tree—in other words, that bagging should be done.

```{r echo=TRUE}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,subset = train, mtry = 12, importance = TRUE)
bag.boston
```

Now to check how well does this bagged model perform on the test set.

```{r echo=TRUE}
yhat.bag <-predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test, main='Plot of observed and predicted value from bagged model of medv for test data',col='skyblue')
abline(0, 1)
```

*Fig. 4 Plot to check the performance of bagged model*

```{r echo=TRUE}
mean((yhat.bag- boston.test)^2)
```

The test set MSE associated with the bagged regression tree is 23.42, about two-thirds of that obtained using an optimally-pruned single tree.

### Random Forest

Here we apply random forest to the Boston data, using same randomForest package in R. Now we use √p variables when building a random forest of classification trees. Here we use mtry=6.

```{r echo=TRUE}
set.seed(1)
rf.boston <-randomForest(medv~., data = Boston, subset = train, mtry = 6, importance = TRUE)
yhat.rf <-predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf- boston.test)^2)
```

The test set MSE is 20.07; this indicates that random forests yielded an improvement over bagging in this case.


We can use the importance() function, to view the importance of each variable. From this function two measures of variable importance are reported. The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees.

```{r echo=TRUE}
# to check importance
importance(rf.boston)
#plot
varImpPlot(rf.boston,main = 'Plots of importance measures')
```

*Fig. 5 Plots of importance measures*

From Fig. 5 we can see that across all of the trees considered in the random forest, the wealth of the community (lstat) and the house size (rm) are by far the two most important variables.

### Boosting

Here we use the gbm package, and within it the gbm() function, to fit boosted gbm() regression trees to the Boston data set.

```{r echo=TRUE}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~., data = Boston[train, ], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
```


From summary we can see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables

```{r echo=TRUE}
plot(boost.boston, i = "rm", main='Partial dependence plot of rm')
```

*Fig. 6 Partial dependence plot of rm *

```{r echo=TRUE}
plot(boost.boston, i = "lstat", main='Partial dependence plot of lstat')
```

*Fig. 7 Partial dependence plot of lstat *

We now use the boosted model to predict medv on the test set:

```{r echo=TRUE}
yhat.boost <- predict(boost.boston, newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost- boston.test)^2)
```


The test MSE obtained is 18.39: this is superior to the test MSE of random forests and bagging.

## Conclusion

We performed fitting of regression tree, bagging, boosting and random forest for Boston data set in MASS package. We observed that test MSE obtained in boosting is minimum among all and lstat and rm are the most important variables. We conclude that larger values of average number of rooms per dwelling (rm), or lower values of the percentage of individuals with lower socioeconomic status (lstat), correspond to more expensive houses.
